# This config file documents all available configurations for training, evaluating or enjoying (watching an agent play in realtime) a model!
# These are the defaults that are used if an incomplete config file was provided via the --config argument used.

### ENVIRONMENT CONFIG ###
environment:
  # Environment Type (Unity, ObstacleTower, Minigrid, Procgen, CartPole)
  type: "Minigrid"
  # type: "Unity"
  # Environment Name (Unity environments have to specify the path to the executable)
  name: "MiniGrid-Empty-Random-6x6-v0"
  # name: "./UnityBuilds/ObstacleTowerReduced/ObstacleTower"
  # How many frames to repeat the same action
  frame_skip: 1
  # Whether to add the last action (one-hot encoded) to the vector observation space
  last_action_to_obs: False
  # Whether to add the last reward to the vector observation space
  last_reward_to_obs: False
  # Number of past observations, which shall be stacked to the current observation (1 means only the most recent observation)
  obs_stacks: 1
  # Whether to convert RGB visual observations to grayscale
  grayscale: False
  # Whether to rescale visual observations to the specified dimensions
  resize_vis_obs: [84, 84]
  # Reset parameters for the environment
  # At minimum, these parameters set the range of training seeds
  # Environments, like Obstacle Tower, provide more parameters to alter the environment
  reset_params:
    start-seed: 0
    num-seeds: 100
    view-size: 3
    max-episode-steps: 96

### MODEL CONFIG ###
model:
  # Whether to load a model
  load_model: False
  # File path to the model
  model_path: "path/to/model.pt"
  # Save the model after every n-th update
  checkpoint_interval: 50
  # Set the to be used activation function (elu, leaky_relu, relu, swish, gelu)
  activation: "relu"
  # Set the to be used encoder
  vis_encoder: "cnn"
  recurrence:
    # Supported recurrent layers: gru, lstm
    layer_type: "lstm"
    # The number of recurrent layers
    num_layers: 1
    # Length of the trained sequences, if set to 0 or smaller the sequence length is dynamically fit to episode lengths
    sequence_length: 32
    # Size of the recurrent layer's hidden state
    hidden_state_size: 128
    # How to initialize the hidden state (zero, one, mean, sample, learned)
    hidden_state_init: "zero"
    # Whether to reset the hidden state before a new episode.
    # Environments that use short episodes are likely to profit from not resetting the hidden state.
    reset_hidden_state: True
    # Wether residual connections should be used for the recurrent layer
    residual: False

  # Set the to be used preprocessing layer
  vec_encoder: "linear"
  num_vec_encoder_units: 128
  # Set the to be used hidden layer
  hidden_layer: "default"
  # Number of hidden layers
  num_hidden_layers: 1
  # Number of hidden units
  num_hidden_units: 512
  
### EVALUATION CONFIG ###
evaluation:
  # Whether to evaluate the model during training
  evaluate: False
  # Number of environments that are used
  n_workers: 3
  # Evaluation seeds (each worker performs on every seed: in this case, overall 30 episodes are used for evaluation (n_workers * seeds))
  seeds:
    start-seed: 100000
    num-seeds: 10
    # Use the explicit-seeds key to override the range of seeds in case of evaluating specific seeds
    # explicit-seeds: [1001, 1002, 1003, 1004, 1005]
  # Evaluate the model after every n-th update during training
  interval: 50

### SAMPLER CONFIG
# The sampler is in charge of collecting training data
# These hyperparameters determine the amount of data and the sampler's behavior
sampler:
  # Number of environments that are used for sampling data
  n_workers: 16
  # Number of steps an agent samples data in each environment (batch_size = n_workers * worker_steps)
  worker_steps: 256

### TRAINER CONFIG ###
trainer:
  # Which algorithm to use. For now, PPO is supported.
  algorithm: "DecoupledPPO"
  # (Optional) Whether the policy shall estimate the advantage function (DAAC algorithm by Raileanu & Fergus, 2021)
  DAAC:
    # Coefficient of the advantage loss
    adv_coefficient: 0.25
  # On which step to resume the training. This affects the hyperparameter schedules only.
  resume_at: 0
  # Discount factor
  gamma: 0.99
  # Regularization parameter used when calculating the Generalized Advantage Estimation (GAE)
  lamda: 0.95
  # Number of PPO update cycles that shall be done (one whole cycle comprises n epochs of m mini_batch updates)
  updates: 1000
  # Number of times that the whole batch of data is used for optimizing the policy using PPO
  # Each epoch trains on a random permutation of the sampled training batch
  policy_epochs: 4
  # Number of mini batches that are trained throughout one policy epoch
  # In case of using a recurrent net, this has to be a multiple of n_workers.
  n_policy_mini_batches: 4
  # Wether to normalize the advantages on minibatch or batch level.
  advantage_normalization: "minibatch"
  # Number of times that the whole batch of data is used for optimizing the value function
  # In contrast to policy epochs, the value function is updated once using the whole data set instead of mini batches.
  value_epochs: 9
  # Number of mini batches that are trained throughout one value epoch
  # In case of using a recurrent net, this has to be a multiple of n_workers.
  n_value_mini_batches: 1
  # Refreshes the buffer every n epochs (-1 = turned off). Stale advantages and hidden states are refreshed.
  refresh_buffer_epoch: -1
  # This interval determines when to optimize the value function based on how many update cycles have passed.
  value_update_interval: 1
  # Strength of clipping the norm of the policy loss gradients
  max_policy_grad_norm: 0.5
  # Strength of clipping the norm of the value loss gradients
  max_value_grad_norm: 0.5
  # Whether the optimization of the policy and the value should be done simultaneously
  # This should be set to False if not enough GPU memory is available
  run_threaded: True
  # Polynomial Decay Schedules
  # Policy Learning Rate
  policy_learning_rate_schedule:
    initial: 3.0e-4
    final: 3.0e-4
    power: 1.0
    max_decay_steps: 1000
  # Value Learning Rate
  value_learning_rate_schedule:
    initial: 3.0e-4
    final: 3.0e-4
    power: 1.0
    max_decay_steps: 1000
  # Beta represents the entropy bonus coefficient
  beta_schedule:
    initial: 0.001
    final: 0.0005
    power: 1.0
    max_decay_steps: 800
  # Strength of clipping the loss of the policy
  policy_clip_range_schedule:
    initial: 0.2
    final: 0.2
    power: 1.0
    max_decay_steps: 1000
  # Strength of clipping the loss of the value function
  value_clip_range_schedule:
    initial: 0.2
    final: 0.2
    power: 1.0
    max_decay_steps: 1000
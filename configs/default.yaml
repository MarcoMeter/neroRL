# This config file documents all available configurations for training, evaluating or enjoying (watching an agent play in realtime) a model!
# These are the defaults that are used if an incomplete config file was provided via the --config argument used.

### ENVIRONMENT CONFIG ###
environment:
  # Environment Type (Unity, ObstacleTower, Minigrid, Procgen, CartPole)
  type: "Minigrid"
  # type: "Unity"
  # Environment Name (Unity environments have to specify the path to the executable)
  name: "MiniGrid-Empty-Random-6x6-v0"
  # name: "./UnityBuilds/ObstacleTowerReduced/ObstacleTower"
  # How many frames to repeat the same action
  frame_skip: 1
  # Whether to add the last action (one-hot encoded) to the vector observation space
  last_action_to_obs: False
  # Whether to add the last reward to the vector observation space
  last_reward_to_obs: False
  # Number of past observations, which shall be stacked to the current observation (1 means only the most recent observation)
  obs_stacks: 1
  # Whether to convert RGB visual observations to grayscale
  grayscale: False
  # Whether to rescale visual observations to the specified dimensions
  resize_vis_obs: [84, 84]
  # Whether to add or concatenate positional encodings to the agent's observation
  positional_encoding: False
  # Reset parameters for the environment
  # At minimum, these parameters set the range of training seeds
  # Environments, like Obstacle Tower, provide more parameters to alter the environment
  reset_params:
    start-seed: 0
    num-seeds: 100
    view-size: 3
    max-episode-steps: 128

### MODEL CONFIG ###
model:
  # Whether to load a model
  load_model: False
  # File path to the model
  model_path: "path/to/model.pt"
  # Save the model after every n-th update
  checkpoint_interval: 50
  # Set the to be used activation function (elu, leaky_relu, relu, swish, gelu)
  activation: "relu"
  # Set the to be used visual encoder
  vis_encoder: "cnn"
  # recurrence:
  #   # Supported recurrent layers: gru, lstm
  #   layer_type: "lstm"
  #   # The number of recurrent layers
  #   num_layers: 1
  #   # Length of the trained sequences, if set to 0 or smaller the sequence length is dynamically fit to episode lengths
  #   sequence_length: 32
  #   # Size of the recurrent layer's hidden state
  #   hidden_state_size: 128
  #   # How to initialize the hidden state (zero, one, mean, sample, learned)
  #   hidden_state_init: "zero"
  #   # Whether to reset the hidden state before a new episode.
  #   # Environments that use short episodes are likely to profit from not resetting the hidden state.
  #   reset_hidden_state: True
  #   # Wether residual connections should be used for the recurrent layer
  #   residual: False
  # Set the to be used vector encoder
  vec_encoder: "linear" # "linear", "none"
  num_vec_encoder_units: 128
  # Set the to be used hidden layer
  hidden_layer: "default"
  # Number of hidden layers
  num_hidden_layers: 1
  # Number of hidden units
  num_hidden_units: 512
  
### EVALUATION CONFIG ###
evaluation:
  # Whether to evaluate the model during training
  evaluate: False
  # Number of environments that are used
  n_workers: 3
  # Evaluation seeds (each worker performs on every seed: in this case, overall 30 episodes are used for evaluation (n_workers * seeds))
  seeds:
    start-seed: 100000
    num-seeds: 10
    # Use the explicit-seeds key to override the range of seeds in case of evaluating specific seeds
    # explicit-seeds: [1001, 1002, 1003, 1004, 1005]
  # Evaluate the model after every n-th update during training
  interval: 50

### SAMPLER CONFIG
# The sampler is in charge of collecting training data
# These hyperparameters determine the amount of data and the sampler's behavior
sampler:
  # Number of environments that are used for sampling data
  n_workers: 16
  # Number of steps an agent samples data in each environment (batch_size = n_workers * worker_steps)
  worker_steps: 256

### TRAINER CONFIG ###
trainer:
  # Which algorithm to use. For now, PPO is supported.
  algorithm: "PPO"
  # On which step to resume the training. This affects the hyperparameter schedules only.
  resume_at: 0
  # Discount factor
  gamma: 0.99
  # Regularization parameter used when calculating the Generalized Advantage Estimation (GAE)
  lamda: 0.95
  # Number of PPO update cycles that shall be done (one whole cycle comprises n epochs of m mini_batch updates)
  updates: 10
  # Number of times that the whole batch of data is used for optimization using PPO
  # Each epoch trains on a random permutation of the sampled training batch
  epochs: 4
  # Refreshes the buffer every n epochs (-1 = turned off). Stale advantages and hidden states are refreshed.
  # This feature might be broken by now
  refresh_buffer_epoch: -1
  # Number of mini batches that are trained throughout one epoch
  # In case of using a recurrent net, this has to be a multiple of n_workers.
  n_mini_batches: 4
  # Wether to normalize the advantages on "minibatch" level, "batch" level or not at all ("no").
  advantage_normalization: "minibatch"
  # Coefficient of the loss of the value function (i.e. critic)
  # It is only of use if model parameters are shared among the actor and the critic
  value_coefficient: 0.25
  # Strength of clipping the loss gradients
  max_grad_norm: 0.5
  # Wether the actor and critic model should share its parameters
  # Decoupled parameter training might be broken by now
  share_parameters: True
  # Polynomial Decay Schedules
  # Learning Rate
  learning_rate_schedule:
    initial: 3.0e-4
    final: 3.0e-4
    power: 1.0
    max_decay_steps: 1000
  # Beta represents the entropy bonus coefficient
  beta_schedule:
    initial: 0.001
    final: 0.0005
    power: 1.0
    max_decay_steps: 800
  # Strength of clipping optimizations done by the PPO algorithm
  clip_range_schedule:
    initial: 0.2
    final: 0.2
    power: 1.0
    max_decay_steps: 1000